################################################################################
#                  _       
#  ___ _ __   __ _| | _____  ___ 
# / __| '_ \ / _` | |/ / _ \/ __|
# \__ \ | | | (_| |   <  __/\__ \
# |___/_| |_|\__,_|_|\_\___||___/
#                 
# Data integration and machine learning pipeline built on Snakemake
#
# https://github.com/khughitt/snakes
#
{% set title_underline = '-' * (config['name'] | length + config['version'] | length + 1) %}
#  {{ config['name'] }} {{ config['version'] }}
#  {{ title_underline }}
#
#  Config : {{ config['config_file'] }}
#  Date   : {{ date_str }}
#
#  Data sources:
{% for dat_name, data_source in config['data_sources'].items() %}
#  - {{ dat_name }}: {{ data_source['path'] }}
{% endfor %}
#
{% set output_dir = '/'.join([config['output_dir'] | expanduser, config['name'] | replace(' ', '_'), config['version']]) %}
#  Output dir: {{ output_dir }} 
#
################################################################################
import glob
import operator
import os
import yaml
import numpy as np
import pandas as pd
from snakes import clustering, filters, gene_sets 

# create output directory, if needed
output_dir = '{{ output_dir }}' 

if not os.path.exists(output_dir):
    os.makedirs(output_dir, mode=0o755)

{#############################################################################################}
{#                                                                                           #}
{# Global variables                                                                          #}
{#                                                                                           #}
{#############################################################################################}
{# list used to keep track of different data sources to use for training set construction #}
{% set training_set_inputs = [] -%}

{# list used to keep track of simple rules that should not be run on a compute cluster #}
{% set simple_rules = [] -%}

{#############################################################################################}
{#                                                                                           #}
{# Default target                                                                            #}
{#                                                                                           #}
{#############################################################################################}
rule all:
    input: "{{ output_dir }}/training_set.csv"

{#############################################################################################}
{#                                                                                           #}
{# Load data
{#                                                                                           #}
{#############################################################################################}
{% for dat_name, data_source in config['data_sources'].items() %}
{% include 'data/' + data_source['data_type'] + '.snakefile' %}
{% endfor %}

{#############################################################################################}
{#                                                                                           #}
{# Combine datasets
{#                                                                                           #}
{#############################################################################################}
#
# Create training set
#
rule combine_datasets:
    input: expand("{{ output_dir }}/data/{processed_datasets}", processed_datasets={{ training_set_inputs }})
    output: "{{ output_dir }}/training_set.csv"
    run:
        # load_feature = lambda x: pd.read_csv(x, index_col=0).T
        def load_processed_dataset(filepath):
          """Loads a single processed dataset and prepares it to be added to training set"""
          dat = pd.read_csv(filepath, index_col=0).T
          dat_name = os.path.basename(filepath).replace('.csv', '')
          dat.columns = dat_name + "_" + dat.columns
          return dat
    
        # combine datasets into a unified <sample> x <variable> dataframe
        pd.concat(map(load_processed_dataset, input), axis=1).to_csv(output[0], index_label='sample_id')

{#############################################################################################}
{#                                                                                           #}
{# Other settings                                                                            #}
{#                                                                                           #}
{#############################################################################################}
{% set localrules_str = ', '.join(simple_rules) %}
localrules: {{ localrules_str }}

{# vim: set softtabstop=2 shiftwidth=2 tabstop=2 filetype=jinja: #}
# vim: set filetype=snakemake: 
