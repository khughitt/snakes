################################################################################
#          _       
#  ___ _ __   __ _| | _____  ___ 
# / __| '_ \ / _` | |/ / _ \/ __|
# \__ \ | | | (_| |   <  __/\__ \
# |___/_| |_|\__,_|_|\_\___||___/
#                 
# Data integration and machine learning pipeline built on Snakemake
#
# https://github.com/khughitt/snakes
#
{% set title_underline = '-' * (config['name'] | length + config['version'] | length + 1) %}
#  {{ config['name'] }} {{ config['version'] }}
#  {{ title_underline }}
#
#  Config : {{ config['config_file'] }}
#  Date   : {{ date_str }}
#
#  Data sources:
{% for dat_name, dat_cfg in config['data_sources'].items() %}
#  - {{ dat_name }}: {{ dat_cfg['path'] }}
{% endfor %}
#
{% set output_dir = '/'.join([config['output_dir'], config['name'] | replace(' ', '_'), config['version']]) %}
#  Output dir: {{ output_dir }} 
#
################################################################################
import glob
import operator
import os
import yaml
import numpy as np
import pandas as pd
from snakes import clustering, filters, gene_sets 

# create output directory, if needed
output_dir = '{{ output_dir }}' 

if not os.path.exists(output_dir):
    os.makedirs(output_dir, mode=0o755)

{#############################################################################################}
{#                                                                                           #}
{# Global variables                                                                          #}
{#                                                                                           #}
{#############################################################################################}
{# list used to keep track of different data sources to use for training set construction #}
{% set training_set_inputs = [] -%}

{# list used to keep track of simple rules that should not be run on a cluster #}
{% set local_rules = [] -%}

{#############################################################################################}
{#                                                                                           #}
{# Default target                                                                            #}
{#                                                                                           #}
{#############################################################################################}
rule all:
    input: "{{ output_dir }}/training_set.csv"

{#############################################################################################}
{#                                                                                           #}
{# Load data
{#                                                                                           #}
{#############################################################################################}
{% for dat_name, dat_cfg in config['data_sources'].items() %}
{% include 'data/' + dat_cfg['type'] + '.snakefile' %}
{% endfor %}

{#############################################################################################}
{#                                                                                           #}
{# Cluster aggregation                                                                       #}
{#                                                                                           #}
{#############################################################################################}
{% for dat_name, dat_cfg in config['data_sources'].items() %}
  {% for clust_params in dat_cfg['clustering'] %}
    {% set clust_method = clust_params['type'] %}
    {% set cur_input = "%s/data/%s.csv" | format(output_dir, dat_name) %}
{% include 'clustering/clustering.snakefile' %}
  {% endfor %}
{% endfor %}


{#############################################################################################}
{#                                                                                           #}
{# Gene set aggregation                                                                      #}
{#                                                                                           #}
{#############################################################################################}
{% set preproccesed_gmt_files = [] %}
{% for dat_name, dat_cfg in config['data_sources'].items() %}
  {% for gene_set_params in dat_cfg['gene_sets'] %}
    {% set gene_set = gene_set_params['type'] %}
    {% for gmt in gene_set_params['gmts'] %}
      {# if multiple gmt files exist for the same gene set, include gmt in rule #}
      {# and filenames                                                          #}
      {% if gene_set_params['gmts'] | length > 1 %}
        {% set gene_set_name = gene_set ~ gmt | basename_no_ext %}
      {% else %}
        {% set gene_set_name = gene_set %}
      {% endif %}
      {# if this is the first time the gene set is being used, include logic for #}
      {# parsing gmt file                                                        #}
      {% set preprocessed_gmt = "%s/gene_sets/%s_%s.gmt" | format(output_dir, gene_set_name, dat_cfg['xid']) %}
      {% if preprocessed_gmt not in preproccesed_gmt_files %}
{% include 'annotations/load_gmt.snakefile' %}
        {% do preproccesed_gmt_files.append(preprocessed_gmt) %}
      {% endif %}
      {% set cur_input = "%s/data/%s.csv" | format(output_dir, dat_name) %}
{% include 'gene_sets/' + gene_set + '.snakefile' %}
    {% endfor %}
  {% endfor %}
{% endfor %}

{#############################################################################################}
{#                                                                                           #}
{# Combine datasets
{#                                                                                           #}
{#############################################################################################}
rule combine_datasets:
    input: expand("{{ output_dir }}/data/{processed_datasets}", processed_datasets={{ training_set_inputs }})
    output: "{{ output_dir }}/training_set.csv"
    run:
        # load_feature = lambda x: pd.read_csv(x, index_col=0).T
        def load_processed_dataset(filepath):
          """Loads a single processed dataset and prepares it to be added to training set"""
          dat = pd.read_csv(filepath, index_col=0).T
          dat_name = os.path.basename(filepath).replace('.csv', '')
          dat.columns = dat_name + "_" + dat.columns
          return dat
    
        # combine datasets into a unified <sample> x <variable> dataframe
        pd.concat(map(load_processed_dataset, input), axis=1).to_csv(output[0], index_label='sample_id')

{#############################################################################################}
{#                                                                                           #}
{# Other settings                                                                            #}
{#                                                                                           #}
{#############################################################################################}
{% set localrules_str = ', '.join(local_rules) %}
localrules: {{ localrules_str }}

{# vim: set softtabstop=2 shiftwidth=2 tabstop=2 filetype=jinja: #}
# vim: set filetype=snakemake: 
